{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with n-grams\n",
    "\n",
    "N-grams are a method for examining the context of words and how words occur together.\n",
    "\n",
    "We will go through some of the most common n-grams, namely bigrams and trigrams, and apply the method to our parliamentary data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "For this analysis, we need pandas and a number of specific modules from the NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ucloud/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures, TrigramAssocMeasures\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "In order to find n-grams in our data we need to format our text to a specific format. We only want to look at the text data so what we need is a big text string. As demonstrated in a [previous notebook](./Loading%20and%20cleaning%20data.ipynb#Single-party-text-extraction) there are two ways to obtain a string with the text data. We can either load the data into a DataFrame and extract all the data from `text` column; or we can load the string from a single text file.\n",
    "\n",
    "To use the DataFrame method, we provide the path to a data file and load it to a DataFrame with pandas' `read_csv()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = '/work/Common-files/Data/Datasæt3/20201.csv' # Path to data file\n",
    "\n",
    "df = pd.read_csv(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract and join the text from the `fulltext_org` column into a string with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_str = ' '.join(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we haven't filtered the data by newspaper and `text_str` contains all the text from this specific year.\n",
    "\n",
    "If we want to focus on a specific party, we only extract the text where the abbreviated party name is present in the `group_name` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "party = 'LA'\n",
    "\n",
    "party_str = ' '.join(df[df['group_name'] == party]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can load the data from a previously saved text file on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = '' # Path to text file\n",
    "\n",
    "with open(text_file) as f:\n",
    "    text_data = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the text string into a list of tokens. Again, we can use the fast but naïve `split()` method or the sophisticated but slower `word_tokenize()` function from NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_tokens = text_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_tokens = word_tokenize(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "This is a good time apply a bit of data cleaning before we proceed.\n",
    "\n",
    "The text may contain a lot of symbols from misread OCR or additional punctuation, which we would like to get rid of. We can filter our tokens and keep only those that are alphabetic - i.e. contains characters. This will also remove any punctuation from our list of tokens - notice, this will only be efficient if have used `word_tokenize()` to create our list of tokens.\n",
    "\n",
    "We iterate over the list of tokens, apply the string method `isalpha()` and append the words that pass the test to a new list of tokens. In order to standardise the text further, we convert all words to lower case when appending to the new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for word in nltk_tokens:\n",
    "    if word.isalpha():\n",
    "        tokens.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This quick cleaning will remove the worst offenders from our analysis. You are welcome to apply other steps of cleaning as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build bigram function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data prepared, we can build the functions that will identify the n-grams in our text. We start with bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigrams_score(word, source, freq_filter=2):\n",
    "    \"\"\"Indentify bigrams that include a specific word.\n",
    "    \n",
    "    Based on a specific target word and a list of tokens (source), the function returns\n",
    "    a list of bigrams including the target word. Optionally the frequency filter can be adjusted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initiate bigram scoring measures\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    \n",
    "    # Generation of bigrams\n",
    "    finder = BigramCollocationFinder.from_words(source)\n",
    "\n",
    "    # Frequency filter which determines the number of times a bigram must occur\n",
    "    finder.apply_freq_filter(freq_filter)  \n",
    "    \n",
    "    # Create filter that discards bigrams without the target word\n",
    "    word_filter = lambda *w: word not in w\n",
    "    \n",
    "    # Application of the word filter\n",
    "    finder.apply_ngram_filter(word_filter)\n",
    "    \n",
    "    # Return bigrams with scores of likelihood\n",
    "    return finder.score_ngrams(bigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function, we first initiate the `BigramAssocMeasures()` function from NLTK. We then initiate the NLTK function `BigramCollocationFinder()` and identify all bigrams in our text source.\n",
    "\n",
    "Next, we create a frequency filter, which sets a threshold of how many times a bigram should appear in the text before we want to keep it. By default, we set the frequency filter to 2.\n",
    "\n",
    "We then create a _lambda_ expression to filter out all bigrams that doesn't contain our target word. Lambda expressions can be thought of as quick, disposable functions that are only needed once. Learn more about lambda expressions and their use cases in the [documentation](https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions).\n",
    "\n",
    "Finally, we apply our filtering lambda expression (`word_filter`) before we return the a list of scored bigrams.\n",
    "\n",
    "### Testing the function\n",
    "Now we can test `bigrams_score` by passing a target word and our list of tokens to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigrams_score('ulovlig', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a list of bigrams which meet our requirements i.e. word filter and frequency filter. They are accompanied by a score, which is a statistical measure of how likely the bigrams are to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build trigram function\n",
    "Similarly, we can build a trigram function in order to identify sequences of three words that occur together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigrams_score(word, source, freq_filter=2):\n",
    "    \"\"\"Indentify trigrams that include a specific word.\n",
    "    \n",
    "    Based on a specific target word and a list of tokens (source), the function returns\n",
    "    a list of trigrams including the target word. Optionally the frequency filter can be adjusted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initiate trigram scoring measures\n",
    "    trigram_measures = TrigramAssocMeasures()\n",
    "    \n",
    "    # Generation of trigrams\n",
    "    finder = TrigramCollocationFinder.from_words(source)\n",
    "\n",
    "    # Frequency filter which determines the number of times a trigram must occur\n",
    "    finder.apply_freq_filter(freq_filter)\n",
    "    \n",
    "    # Create filter that discards trigrams without the target word\n",
    "    word_filter = lambda *w: word not in w\n",
    "    \n",
    "    # Application of the word filter\n",
    "    finder.apply_ngram_filter(word_filter)\n",
    "\n",
    "    # Return bigrams with scores of likelihood\n",
    "    return finder.score_ngrams(trigram_measures.likelihood_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_score('raske', tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "We can process our data in various ways in order to optimise our n-grams. One of them is stemming.\n",
    "\n",
    "Stemming is the process of removing all inflections of a word so only the word stem remains.\n",
    "\n",
    "To stem our words, we need a stemmer. NLTK includes an implementation of the [Snowball stemmer](https://snowballstem.org/), which is based on the original Porter stemmer.\n",
    "\n",
    "First we need to initiate the stemmer; then we iterate over our tokens, stem each word and append it to a new list of stemmed tokens. Because the stemming process is language specific, we need to pass the language of our text to the stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = 'danish'\n",
    "\n",
    "stemmer = SnowballStemmer(language)\n",
    "\n",
    "stemmed_tokens = []\n",
    "\n",
    "for word in tokens:\n",
    "    stem = stemmer.stem(word)\n",
    "    stemmed_tokens.append(stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass our list of stemmed tokens to the n-gram functions and inspect how the output is affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_score('sprog', stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams_score('kritisk', stemmed_tokens, freq_filter=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have demonstrated how we can build our own tools to identify n-grams. We have encountered some of the more technical aspects of NLTK but by enclosing our analyses into functions, we can fairly easily get to the results.\n",
    "\n",
    "We have also encountered stemming, which can be a useful tool whenever we want to count words across a text. We also applied a few other techniques for optimising our analyses. Namely, we cleaned the data by removing punctuation and OCR noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
