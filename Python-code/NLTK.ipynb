{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NLTK - Natural Language Toolkit\n",
    "\n",
    "Natural Language Toolkit (NLTK) is a Python library that allows us to easily perform various text analyses. NLTK includes a massive amount of useful tools. In this notebook, we will look at a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "We start by importing the NLTK library and the pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If NLTK is not installed, we can install it by typing `pip install nltk` into a terminal or by using the command below directly from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When NLTK is installed and imported, we can download additional material to extend the functionality of the library. NLTK includes a great number of corpora, models, stopword list and other Natural Language Processing (NLP) tools.\n",
    "\n",
    "We can either download specific parts of the tools or we can download everything at once with the command below. Notice that it is only necessary to download additional data the first time the NLTK installation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Once again we need our text data in the form of a text string. We can either load the data from a CSV-file and extract the text data or load it directly from a text file. For more details on this process, see the [N-grams notebook](./N-grams.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_file= '/work/Common-files/Data/Datas√¶t3/20191.csv' # Path to data file\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "\n",
    "text_str = ' '.join(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = '' # Path to text file\n",
    "\n",
    "with open(text_file) as f:\n",
    "    text_str = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "In order to work with our text data, we need to process our text string a bit.\n",
    "\n",
    "First, we convert the text into a list of tokens with the NLTK `word_tokenize()` function. We also create a NLTK `Text` object which allows us to apply various NLTK methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Text` object is created from the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK methods\n",
    "\n",
    "With our `Text` object we can perform a number of text analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `count()`\n",
    "The simplest method is `count()`, which returns the count of a specific term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.count('grundloven')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `collocation_list()`\n",
    "Similar to the n-grams notebook, `collocation_list()` returns a list of the most common word pairs in the text. Notice, that in some versions of Python `collocation_list()` doesn't work. If this is the case, try `collocations()` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `concordance()`\n",
    "The `concordance()` method returns the context of a specific term. The length of the output can be modified with the `width` and `lines` keyword-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.concordance('samfundssind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `similar()`\n",
    "In order to identify words that appear in a similar context, we can use the `similar()` method. This can sometimes be useful if we want to look for common OCR errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "text.similar('lovgivning')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `dispersion_plot()`\n",
    "The `dispersion_plot()` method lets us visualise how terms occur across our text. If the text has a temporal aspect - as our parliamentary data sorted by date - `dispersion_plot()` can approximate a timeline. However, the linearity can be somewhat skewed depending on how evenly spread the text data are.\n",
    "\n",
    "The method accepts a list of one or more terms as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "terms = ['epidemi', 'pandemi', 'grundloven', 'mink', 'samfundssind']\n",
    "\n",
    "text.dispersion_plot(terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distribution\n",
    "Frequency distribution is another useful tool built into NLTK which gives us a quick overview of the most common words in our text.\n",
    "\n",
    "We generate the frequency distribution from our list of tokens with the `FreqDist()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then inspect the most common words. The `most_common()` method returns a number of the most common tokens and how many times they appear in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The immediate results are not very interesting. This is a good time for some data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stop words\n",
    "Our initial results included a lot of short, uninteresting words. These are commonly known as stop words. We can exclude these from our analysis by applying a list of stop words.\n",
    "\n",
    "For this purpose, NLTK has a built-in list of stop words that we can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('danish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter the list of tokens against the stop words. First, we create a new, empty list for the filtered tokens. We then iterate over the list of tokens and for each word, we check if the word is in the stop words list. To get rid of punctuation, we also check if the word consists of characters with the string method `isalpha()`. If these two conditions are met, we append the word to our new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tokens = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.lower() not in stopwords and word.isalpha():\n",
    "        filtered_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a new frequency distribution from the filtered tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered = nltk.FreqDist(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are better than before but we still have a lot of uninteresting words.\n",
    "\n",
    "### Custom stop words list\n",
    "Whenever the NLTK stop words list is insufficient we can supply our own list of stop words which can be tailored to a specific domain.\n",
    "\n",
    "We load a custom stop words list from a text file and save it the variable `my_stopwords`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords_file = '/work/Common-files/Diverse/dk_stopord.txt' # Path to text file with stop words.\n",
    "\n",
    "with open(stopwords_file) as f:\n",
    "    my_stopwords = f.read().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new list for words filtered against our custom stop words list.\n",
    "\n",
    "Notice, that we also convert each word to lower case in order to catch more stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = []\n",
    "\n",
    "for word in tokens:\n",
    "    if word.lower() not in my_stopwords and word.isalpha():\n",
    "        clean_tokens.append(word.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new frequency distributions and inspect the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_clean = nltk.FreqDist(clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_clean.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are better but not perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word length\n",
    "Most of the common words in our filtered text are short and does not carry a lot of meaning. If we assume that short words in general are uninteresting, we can filter our text again and only keep words above a certain length.\n",
    "\n",
    "Below create a new list of tokens and only keep words with a length above six characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_tokens = []\n",
    "\n",
    "for word in filtered_tokens:\n",
    "    if len(word) > 6:\n",
    "        long_tokens.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a frequency distribution of the long words and inspect the most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_long = nltk.FreqDist(long_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_long.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our results are somewhat interesting, as we have a lot more meaningful words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting frequency distribution\n",
    "For a visual representation of the frequency distribution we can use the `plot()` method. We supply the method with the number of words we want to include.\n",
    "\n",
    "Notice, that if we call the method without a number, Python will attempt to include all unique words in the text, which will be a very taxing operation without any value to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_long.plot(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a title to our plot with the `title` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_long.plot(30, title='Most common words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also count the terms cumulatively with the `cumulative` keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist_long.plot(30, title='Most common words (Cumulative)', cumulative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap up\n",
    "\n",
    "NLTK is a very large and powerful Python library and the possibilities are virtually endless. In this notebook, we have scratched the surface and demonstrated some of the tools.\n",
    "\n",
    "We have converted our data to a `Text` object which allows us to perform a number analyses with little effort. We have also worked with frequency distributions and refined our data for a better result.\n",
    "\n",
    "The examples in this notebook are adapted from the book [Natural Language Processing with Python](https://www.nltk.org/book/), which is a recommended resource if you want to learn more about NLTK."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
